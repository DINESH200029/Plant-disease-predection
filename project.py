# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DLHHeGDxA-8QvHx4qv_tRQPZ4oS3pgLc
"""

import mahotas
print("IMPORTED")

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
from matplotlib.colors import hsv_to_rgb
from scipy import ndimage
import numpy as np
import h5py
import os
import glob
import cv2
import warnings
from matplotlib import pyplot as plt
from matplotlib import pyplot
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.model_selection import KFold, StratifiedKFold
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC


print("IMPORTED")

images_per_class       = 90
fixed_size             = tuple((500, 500))


train_path             = "/Users/dineshpathakoti/Desktop/leaf_major/datasetN/train"
h5_train_data          = "/Users/dineshpathakoti/Desktop/leaf_major/output/train_data.h5"
h5_train_labels        = "/Users/dineshpathakoti/Desktop/leaf_major/output/train_labels.h5"
bins                   = 8

# Converting each image to RGB from BGR format

def rgb_bgr(image):

    rgb_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    return rgb_img

# Conversion to HSV image format from RGB

def bgr_hsv(rgb_img):
    hsv_img = cv2.cvtColor(rgb_img, cv2.COLOR_RGB2HSV)
    return hsv_img

# image segmentation

# for extraction of green and brown color


def img_segmentation(rgb_img,hsv_img):
    lower_green = np.array([25,0,20])
    upper_green = np.array([100,255,255])
    healthy_mask = cv2.inRange(hsv_img, lower_green, upper_green)
    result = cv2.bitwise_and(rgb_img,rgb_img, mask=healthy_mask)

    lower_white = np.array([0,0,0])
    upper_white = np.array([255,255,255])
    di_mask = cv2.inRange(hsv_img, lower_green, upper_green)
    result = cv2.bitwise_and(rgb_img,rgb_img, mask=healthy_mask)

    lower_brown = np.array([10,0,10])
    upper_brown = np.array([30,255,255])
    disease_mask = cv2.inRange(hsv_img, lower_brown, upper_brown)
    disease_result = cv2.bitwise_and(rgb_img, rgb_img, mask=disease_mask)

    final_mask = healthy_mask + disease_mask+di_mask
    final_result = cv2.bitwise_and(rgb_img, rgb_img, mask=final_mask)
    return final_result


# Hu Moments
def fd_hu_moments(image):
    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    feature = cv2.HuMoments(cv2.moments(image)).flatten()
    return feature


# Haralick Texture
def fd_haralick(image):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    haralick = mahotas.features.haralick(gray).mean(axis=0)
    return haralick


# Color
def fd_histogram(image, mask=None):
    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
    hist  = cv2.calcHist([image], [0, 1, 2], None, [bins, bins, bins], [0, 256, 0, 256, 0, 256])
    cv2.normalize(hist, hist)
    return hist.flatten()

print("Completed")

# get the training labels
#train_labels = os.listdir(train_path) #for windows

train_labels = [f for f in os.listdir(train_path) if not f.startswith('.')]


# sort the training labels
train_labels.sort()
print(train_labels)

# empty lists to hold feature vectors and labels
global_features = []
labels          = []

'''
# rename files
import os
for c,i in enumerate(os.listdir()):
    os.rename(i,""+str(c)+".JPG")
'''


for training_name in train_labels:

    dir = os.path.join(train_path, training_name)

    # get the current training label
    current_label = training_name


    for x in range(1,images_per_class+1):
        # get the image file name
        try:
            file = dir + "/" + str(x) + ".png"

        # read the image and resize it to a fixed-size
            image = cv2.imread(file)

            image = cv2.resize(image, fixed_size)

        except:
            print(file + "not good")
        # preprocessing and segmentation

        RGB_BGR       = rgb_bgr(image)
        BGR_HSV       = bgr_hsv(RGB_BGR)
        IMG_SEGMENT   = img_segmentation(RGB_BGR,BGR_HSV)

        # Call for Global Fetaure Descriptors

        fv_hu_moments = fd_hu_moments(IMG_SEGMENT)
        fv_haralick   = fd_haralick(IMG_SEGMENT)
        fv_histogram  = fd_histogram(IMG_SEGMENT)

        # Concatenate

        global_feature = np.hstack([fv_histogram, fv_haralick, fv_hu_moments])



        # update the list of labels and feature vectors
        labels.append(current_label)
        global_features.append(global_feature)

    print("[STATUS] processed folder: {}".format(current_label))

print("[STATUS] completed Global Feature Extraction...")

# get the overall feature vector size
print("[STATUS] feature vector size {}".format(np.array(global_features).shape))

# get the overall training label size
print("[STATUS] training Labels {}".format(np.array(labels).shape))

# encode the target labels
targetNames = np.unique(labels)
le          = LabelEncoder()
target      = le.fit_transform(labels)
print("[STATUS] training labels encoded...")

# scale features in the range (0-1)
from sklearn.preprocessing import MinMaxScaler
scaler            = MinMaxScaler(feature_range=(0, 1))
rescaled_features = scaler.fit_transform(global_features)
print("[STATUS] feature vector normalized...")

print("[STATUS] target labels: {}".format(target))
print("[STATUS] target labels shape: {}".format(target.shape))

# save the feature vector using HDF5
h5f_data = h5py.File(h5_train_data, 'w')
h5f_data.create_dataset('dataset_1', data=np.array(rescaled_features))

h5f_label = h5py.File(h5_train_labels, 'w')
h5f_label.create_dataset('dataset_1', data=np.array(target))

h5f_data.close()
h5f_label.close()

# training



warnings.filterwarnings('ignore')


num_trees = 100
test_size = 0.25
seed      = 9
train_path = "/Users/dineshpathakoti/Desktop/leaf_major/datasetN/train"
test_path  = "/Users/dineshpathakoti/Desktop/leaf_major/datasetN/test"
h5_train_data    = '/Users/dineshpathakoti/Desktop/leaf_major/output/train_data.h5'
h5_train_labels    = '/Users/dineshpathakoti/Desktop/leaf_major/output/train_labels.h5'
scoring    = "accuracy"

# get the training labels
#train_labels = os.listdir(train_path) #for windows

train_labels =  [f for f in os.listdir(train_path) if not f.startswith('.')]


# sort the training labels
train_labels.sort()

if not os.path.exists(test_path):
    os.makedirs(test_path)

# create all the machine learning models
models = {}
models['KNN'] =KNeighborsClassifier()
models['RF']=RandomForestClassifier(n_estimators=num_trees, random_state=seed)
models['NB']=GaussianNB()
models['SVM']=SVC(kernel='rbf',random_state=seed)

# variables to hold the results and names
results = []
names   = []

# import the feature vector and trained labels
h5f_data  = h5py.File(h5_train_data, 'r')
h5f_label = h5py.File(h5_train_labels, 'r')

global_features_string = h5f_data['dataset_1']
global_labels_string   = h5f_label['dataset_1']

global_features = np.array(global_features_string)
global_labels   = np.array(global_labels_string)

h5f_data.close()
h5f_label.close()

# verify the shape of the feature vector and labels
print("[STATUS] features shape: {}".format(global_features.shape))
print("[STATUS] labels shape: {}".format(global_labels.shape))

print("[STATUS] training started...")

# split the training and testing data
(trainDataGlobal, testDataGlobal, trainLabelsGlobal, testLabelsGlobal) = train_test_split(np.array(global_features),
                                                                                          np.array(global_labels),
                                                                                          test_size=test_size,
                                                                                          random_state=seed)

print("[STATUS] splitted train and test data...")
print("Train data  : {}".format(trainDataGlobal.shape))
print("Test data   : {}".format(testDataGlobal.shape))

k4_ac=[]
k4_sd=[]
k6_ac=[]
k6_sd=[]
# k-fold cross validation
for model in models:
    kfold = KFold(n_splits=12, random_state=seed,shuffle=True)
    cv_results = cross_val_score(models[model],np.array(global_features),np.array(global_labels), cv=kfold, scoring=scoring)
    results.append(cv_results)
    names.append(model)
    k4_ac.append(cv_results.mean()*100)
    k4_sd.append(cv_results.std())
    msg = "%s: %f (%f)" % (model, cv_results.mean(), cv_results.std())
    print(msg)



results = []
names   = []
# k-fold cross validation
for model in models:
    kfold = KFold(n_splits=8, random_state=seed,shuffle=True)
    cv_results = cross_val_score(models[model], trainDataGlobal, trainLabelsGlobal, cv=kfold, scoring=scoring)
    results.append(cv_results)
    names.append(model)
    k6_ac.append(cv_results.mean()*100)
    k6_sd.append(cv_results.std())
    msg = "%s: %f (%f)" % (model, cv_results.mean(), cv_results.std())
    print(msg)


results = []
names   = []
k_ac=[]
k_sd=[]

# k-fold cross validation
for model in models:
    kfold = KFold(n_splits=10, random_state=seed,shuffle=True)
    cv_results = cross_val_score(models[model], trainDataGlobal, trainLabelsGlobal, cv=kfold, scoring=scoring)
    results.append(cv_results)
    names.append(model)
    k_ac.append(cv_results.mean()*100)
    k_sd.append(cv_results.std())
    msg = "%s: %f (%f)" % (model, cv_results.mean(), cv_results.std())
    print(msg)

fig = plt.figure()
plt.bar(names,k_ac,label="k=10" ,width = 0.3,)
plt.bar(np.arange(len(names))+0.3,k6_ac,label="k=8" ,width = 0.3)

plt.bar(np.arange(len(names))+0.6,k4_ac,label="k=12" ,width = 0.3)
leg = plt.legend()

plt.xlabel("models")
plt.ylabel("Accuracy")
plt.title("Accuracies of all models")
plt.show()

fig = plt.figure()
plt.bar(names,k_sd,label="k=10" ,width = 0.3,)
plt.bar(np.arange(len(names))+0.3,k6_sd,label="k=8" ,width = 0.3)

plt.bar(np.arange(len(names))+0.6,k4_sd,label="k=12" ,width = 0.3)
leg = plt.legend()
plt.xlabel("models")
plt.ylabel("sd")
plt.title("Standard Deviation")
plt.show()


test_acc=[]
train_acc=[]

c=0
for model in models:

    models[model].fit(trainDataGlobal, trainLabelsGlobal)
    y_predict=models[model].predict(testDataGlobal)
    y_predict
    print(y_predict)
    y_train_pre = models[model].predict(trainDataGlobal)
    cm = confusion_matrix(testLabelsGlobal,y_predict)
    print(cm)
    import seaborn as sns
    sns.heatmap(cm ,annot=True)

    print(model)
    print(classification_report(testLabelsGlobal,y_predict))
    from sklearn.metrics import accuracy_score
    st = accuracy_score(trainLabelsGlobal, y_train_pre)
    train_acc.append(st*100)
    sa = accuracy_score(testLabelsGlobal, y_predict)
    test_acc.append(sa*100)

print(test_acc)


print(train_acc)

fig = plt.figure()
plt.bar(names,test_acc,label="Test Acc" ,width = 0.3)
plt.bar(np.arange(len(names))+0.3,train_acc,label="Train Acc" ,width = 0.3)

leg = plt.legend()

plt.xlabel("models")
plt.ylabel("Accuracy")
plt.title("Train and test Accuracies of all models 75:25 split")
plt.show()

#Testing using new Image


img = cv2.imread('\\Users\\dineshpathakoti\\Desktop\\leaf_major\\datasetN\\train\\bacterial leaf spot-edited\\2.png')


img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
plt.imshow(img)
plt.show()

hsv_img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)
plt.imshow(hsv_img)

lower_green = np.array([25,0,20])
upper_green = np.array([100,255,255])
mask = cv2.inRange(hsv_img, lower_green, upper_green)
result = cv2.bitwise_and(img, img, mask=mask)
plt.subplot(1, 2, 1)
plt.imshow(mask, cmap="gray")
plt.subplot(1, 2, 2)
plt.imshow(result)

plt.show()

lower_brown = np.array([10,0,10])
upper_brown = np.array([30,255,255])
disease_mask = cv2.inRange(hsv_img, lower_brown, upper_brown)
disease_result = cv2.bitwise_and(img, img, mask=disease_mask)
plt.subplot(1, 2, 1)
plt.imshow(disease_mask, cmap="gray")
plt.subplot(1, 2, 2)

plt.imshow(disease_result)
plt.show()


final_mask = mask + disease_mask
final_result = cv2.bitwise_and(img, img, mask=final_mask)
plt.figure(figsize=(10,10))
plt.subplot(1, 2, 1)
plt.imshow(final_mask, cmap="gray")
plt.subplot(1, 2, 2)
plt.imshow(final_result)
plt.show()

# Feature Descripotor

fv_hu_moments = fd_hu_moments(final_result)
fv_haralick   = fd_haralick(final_result)
fv_histogram  = fd_histogram(final_result)

# Concatenate
global_features = []
global_feature = np.hstack([fv_histogram, fv_haralick, fv_hu_moments])
global_features.append(global_feature)



from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(0, 1))
rescaled_features = scaler.fit_transform(global_features)
np.array(global_features).shape

print("[STATUS] feature vector size {}".format(np.array(global_features).shape))



y_predict1=models["RF"].predict(np.array(global_features))

print(y_predict1)

print("RF")
dict={
    0:"bacterial",
    1:"curl",
    2:"fusarium",
    3:"healthy",
    4:"pested"

}

print(dict[y_predict1[0]])

